

from itertools import product, combinations
import numpy as np
import networkx as nx


#### __init__ 2015-06-11
def get_characterizers(i, k, neighs, type_arr, reindices,
                        agg_desc=None):
    """Retrieve local characterizers for i and k.
    """
    if agg_desc is None:
        val_i = type_arr[reindices[i, k]]
        neighs_k = reindices[neighs, k]
        vals = type_arr[neighs_k]
    else:
        val_i = type_arr[reindices[i, k]]
        neighs_k = neighs
        #neighs_k = reindices[neighs, k]
        vals = agg_desc[neighs, :]
    return val_i, neighs_k, vals



#### pjensen 2015-06-08


#    def compute_local_measure(self, val_i, vals, n_vals, N_x, bool_extra_out):
#        """Main function to compute the local descriptor measure of the
#        desired point.
#        It returns the part of the measure computed with local information.
#        """
#        ## Computation of descriptors
#        corr_loc_i, counts_i =\
#            computation_of_counts([vals, val_i, n_vals, N_x])
#        ## Return counts if it is desired
#        if bool_extra_out:
#            return corr_loc_i, counts_i
#        else:
#            return corr_loc_i


#    def compute_local_descriptors(self, val_i, vals, n_vals, N_x, C, idx_null):
#        """Function to compute the descriptor for each point individually. It
#        is usefull for the ML approach in which we have to compute the matrix
#        of data and we represent each point with the representation desired.
#        """
#        ## Computation of descriptors
#        corr_loc_i, counts_i =\
#            computation_of_counts([vals, val_i, n_vals, N_x])
#        ## Transform counts into descriptors
#        descriptors = np.log10(np.multiply(C, corr_loc_i))
#        descriptors[idx_null] = 0.
#        return descriptors

#    def retrieve_and_compute_measure():
#        """Retrieve the local descriptors and compute the
#        """
#        pass

#    def compute_from_descriptors(self, vals, val_i, n_vals, N_x):
#        """Main function to compute from descriptors aggregated in a file.
#        """
#        counts_i = np.sum(vals, axis=0)
#        corr_loc_i = compute_loc_M_index(counts_i, val_i, n_vals, N_x)
#        return corr_loc_i





####



def init_measure_compute(df, type_vars, loc_vars, radius, permuts):
    """Auxiliary function to prepare the initialization and preprocess of the
    required input variables.
    """
    # Values
    type_vals = list(df[type_vars].unique())
    type_vals = sorted(type_vals)

    # Global stats
    N_t = df.shape[0]
    N_x = [np.sum(df[type_vars] == type_v) for type_v in type_vals]
    N_x = np.array(N_x)

    n_vals = len(type_vals)
    repl = dict(zip(type_vals, range(n_vals)))

    type_arr = np.array(df[type_vars].replace(repl)).astype(int)
    type_arr = type_arr if len(type_arr) == 2 else type_arr.reshape((N_t, 1))

    # Preparing reindices
    reindex = np.array(df.index)
    reindex = reindex.reshape((N_t, 1))
    if permuts is not None:
        if type(permuts) == int:
            permuts = [np.random.permutation(N_t) for i in range(permuts)]
            permuts = np.vstack(permuts).T
            bool_ch = len(permuts.shape) == 1
            permuts = permuts.reshape((N_t, 1)) if bool_ch else permuts
        n_per = permuts.shape[1]
        permuts = [reindex[permuts[:, i]] for i in range(n_per)]
        permuts = np.hstack(permuts)
    reindex = [reindex] if permuts is None else [reindex, permuts]
    reindices = np.hstack(reindex)
    n_calc = reindices.shape[1]

    # Computation of the locations
    locs = df[loc_vars].as_matrix()
    # indices
    indices = np.array(df.index)

    ## Radius computation (TODO: self.bool_r_array)
    if type(radius) == float:
        radius = radius/6371.009
    elif type(radius) == np.ndarray:
        radius = radius/6371.009
    elif type(radius) == str:
        radius = np.array(df[radius])/6371.009

    output = (type_arr, type_vals, n_vals, N_t, N_x, reindices,
              n_calc, locs, indices)
    return output




def computation_neighbourhood(i, df, loc_vars, type_var, type_vals):
    """"""
    indices = get_from_neighbourhood(point, coordinates)
    neighs = df.irow(indices)
    n_t = neighs.shape[0]

    if type(type_vals) != list:
        ns = np.sum(neighs == type_vals)
    else:
        ns = []
        for i in range(len(type_vals)):
            n_aux = np.sum(neighs == type_vals[i])
            ns.append(n_aux)
    return ns, n_t


def get_from_neighbourhood(point_i, coordinates):
    pass


def get_from_type(df, type_var, type_val):
    """Function to retrieve the indices of the rows with type equal to
    type_val.
    """
    indices = df[typevar][df[typevar] == type_val].index
    return indices


def self_interaction(df, loc_vars, type_var, type_val):
    """Computation of the self interactions."""
    ## 0. Computation of needed variables
    idxs = get_from_type(df, type_var, type_val)
    N_t = df.shape[0]
    N_a = idxs.shape[0]

    ## 1. Computation of the index
    C = np.log10((N_t-1)/float(N_a*(N_a-1)))
    suma = 0
    for i in idxs:
        n_a, n_t = computation_neighbourhood(i, df, loc_vars, type_var,
                                             type_val)
        suma = suma + n_a/float(n_t)
    a_AA = C*suma
    return a_AA


def x_interaction(df, loc_vars, type_var, type_vals):
    """Computation of the interaction between """
    ## 0. Computation of needed variables
    idxs_A = get_from_type(df, type_var[0])
    idxs_B = get_from_type(df, type_var[1])

    N_t = locations.shape[0]
    N_a = idxs_A.shape[0]
    N_b = idxs_B.shape[0]

    ## 1. Computation of the index
    C = np.log10((N_t-N_a)/float(N_a*N_b))
    suma = 0
    for i in idxs:
        ns, n_t = computation_neighbourhood(i, df, type_vals)
        n_a, n_b = ns
        suma = suma + n_b/float((n_t-n_a))

    a_AB = C*suma
    return a_AB


def built_network(df, loc_vars, type_var):
    """Function for building the network from the locations."""

    type_vals = list(df[type_var].unique())
    n_vals = len(type_vals)
    pairs = product(range(n_vals), range(n_vals))

    net = np.zeros((n_vals, n_vals))
    for p in pairs:
        if p[0] == p[1]:
            type_val = type_vals[p[0]]
            aux = self_interaction(df, loc_vars, type_var, type_val)
        else:
            type_val_pairs = [type_vals[p[0]], type_vals[p[1]]]
            aux = x_interaction(df, loc_vars, type_var, type_val_pairs)
        net[p[0], p[1]] = aux

    return net


def get_zetas(net, clusters):
    ## TODO with networkx
    for c in clusters:
        n_c = len(c)
        pairs = combinations(2, n_c)
        values = [net[p[0], p[1]] for p in pairs]
        pos_values = [val for val in values if val >= 0]
        neg_values = [val for val in values if val >= 0]
    pass



def local_jensen_corr_from_neighs(cnae_arr, neighs, type_vals, permuts):
    """"""
    ## Global variables
    n_vals = len(type_vals)
    indices = np.array(neighs.index)
    corr_loc = np.zeros((n_vals, n_vals))
    for j in xrange(indices.shape[0]):
        ## Retrieve neighs from neighs dataframe
        neighs_j = neighs.loc[indices[j], 'neighs'].split(',')
        neighs_j = [int(e) for e in neighs_j]
        vals = cnae_arr[neighs_j]
        ## Count the number of companies of each type
        counts_j = np.array([np.sum(vals == v) for v in range(len(type_vals))])
        cnae_val = cnae_arr[indices[j]]
        idx = cnae_val
        ## Compute the correlation contribution
        tot = counts_j.sum()
        counts_j[idx] -= 1
        if counts_j[idx] == tot:
            corr_loc_j = np.zeros(n_vals)
            corr_loc_j[idx] = counts_j[idx]/tot
        else:
            corr_loc_j = counts_j/(tot-counts_j[idx])
            corr_loc_j[idx] = counts_j[idx]/tot
        ## Aggregate to local correlation
        corr_loc[idx, :] += corr_loc[idx, :]
    return corr_loc


def local_jensen_corr_from_neighs(cnae_arr, neighs, type_vals, reindices):
    """"""
    ## Global variables
    n_vals = len(type_vals)
    n_calc = reindices.shape[1]
    indices = np.array(neighs.index)
    corr_loc = np.zeros((n_vals, n_vals, n_calc))
    for j in xrange(indices.shape[0]):
        ## Retrieve neighs from neighs dataframe
        neighs_j = neighs.loc[indices[j], 'neighs'].split(',')
        neighs_j = [int(e) for e in neighs_j]
        vals = cnae_arr[neighs_j]
        ## Count the number of companies of each type
        counts_j = np.array([np.sum(vals == v) for v in range(len(type_vals))])
        cnae_val = cnae_arr[indices[j]]
        idx = cnae_val
        ## Compute the correlation contribution
        tot = counts_j.sum()
        counts_j[idx] -= 1
        if counts_j[idx] == tot:
            corr_loc_j = np.zeros(n_vals)
            corr_loc_j[idx] = counts_j[idx]/tot
        else:
            corr_loc_j = counts_j/(tot-counts_j[idx])
            corr_loc_j[idx] = counts_j[idx]/tot
        ## Aggregate to local correlation
        corr_loc[idx, :] += corr_loc[idx, :]
    return corr_loc

            for k in range(n_calc):
                #val_i = df.loc[reindices[i, k], type_var]
                val_i = cnae_arr[reindices[i, k]]
                neighs_k = reindices[neighs, k]
                vals = cnae_arr[neighs_k]
                ## Count the number of companies of each type
                nv = n_vals
                c = [np.count_nonzero(np.equal(vals, v)) for v in range(nv)]
                counts_i = np.array(c)
                idx = val_i
                ## Compute the correlation contribution
                counts_i[idx] -= 1
                tot = counts_i.sum()
                if counts_i[idx] == tot:
                    corr_loc_i = np.zeros(n_vals)
                    corr_loc_i[idx] = counts_i[idx]/tot
                else:
                    corr_loc_i = counts_i/(tot-counts_i[idx])
                    corr_loc_i[idx] = counts_i[idx]/tot
                ## Aggregate to local correlation
                corr_loc[idx, :, k] += corr_loc_i







#######################################################################################
#######################################################################################
### REFACTORING CLASS
#######################################################################################
#######################################################################################
#######################################################################################






    def compute_nets_sequ_data(self):
        """Main abstract function for compute matrix of spatial correlation
        from data in a sequential mode.
        """
        n_vals, n_calc, df, N_t, N_x, radius, locs, kdtree, type_arr, reindices, bool_inform = aux
        ## 1. Computation of the local spatial correlation with a given measure
        corr_loc = np.zeros((n_vals, n_vals, n_calc))
        indices = np.array(df.index)
        ## Begin to track the process
        t0 = time.time()
        bun = 0
        for i in xrange(N_t):
            # Check radius
            if type(radius) == np.ndarray:
                r = radius[i]
            ## Obtaining neighs of a given point
            point_i = locs[indices[i], :]
            neighs = kdtree.query_ball_point(point_i, r)
            ## Loop over the possible reindices
            for k in range(n_calc):
                # Retrieve local characterizers
                val_i = type_arr[reindices[i, k]]
                neighs_k = reindices[neighs, k]
                vals = type_arr[neighs_k]
                # Computation of the local measure
                corr_loc_i = self.compute_local_measure(vals, val_i, n_vals,
                                                        N_x)
                # Aggregation
                corr_loc[val_i, :, k] += corr_loc_i
            ## Finish to track this process
            if bool_inform and (i % self.lim_rows) == 0 and i != 0:
                t_sp = time.time()-t0
                bun += 1
                self.logfile.write_log(message2a % (bun, self.lim_rows, t_sp))
                t0 = time.time()
        return corr_loc

    def compute_nets_sequ_neighs():
        """Main function to perform spatial correlation computation."""
        n_vals, n_calc, df, N_t, N_x, radius, locs, kdtree, type_arr, reindices, bool_inform = aux

        ## 1. Computation of local spatial correlations
        corr_loc = np.zeros((n_vals, n_vals, n_calc))
        for f in self.neighs_files:
            ## Begin to track the process
            self.logfile.write_log(message1 % (f.split('/')[-1]))
            t0 = time.time()
            ## Read the file of neighs
            neighs = pd.read_csv(f, sep=';', index_col=0)
            ## Compute corr with these neighs
            indices = np.array(neighs.index)
            corr_loc_f = np.zeros((n_vals, n_vals, n_calc))
            counts_f = np.zeros((n_vals, n_vals, n_calc))
            for j in xrange(indices.shape[0]):
                ## Retrieve neighs from neighs dataframe
                neighs_j = neighs.loc[indices[j], 'neighs'].split(',')
                neighs_j = [int(e) for e in neighs_j]
                # Compute measure with random permutations possibilities
                for k in range(n_calc):
                    # Retrieve local characterizers
                    val_i = type_arr[reindices[j, k]]
                    neighs_k = reindices[neighs_j, k]
                    vals = type_arr[neighs_k]
                    # Computation of the local measure
                    corr_loc_j = self.compute_local_measure(vals, val_i,
                                                            n_vals, N_x)
                corr_loc_f += corr_loc_j
            corr_loc += corr_loc_f
            ## Finish to track this process
            self.logfile.write_log(message2 % (time.time()-t0))
            del neighs
        return corr_loc

    def compute_nets_sequ_aggs():
        """Main function to perform spatial correlation computation in a
        sequential mode using aggregated information given by a '''''file'''''.
        """
        n_vals, n_calc, df, N_t, N_x, radius, locs, kdtree, type_arr, reindices, bool_inform = aux

        # KDTree retrieve object instantiation
        locs2 = df2[loc_vars2].as_matrix()
        kdtree = KDTree(locs2, leafsize=100)
        agg_desc = df2[agg_desc_vars].as_matrix()

        ## 1. Computation of local spatial correlations
        corr_loc = np.zeros((n_vals, n_vals, n_calc))
        indices = np.array(df.index)
        ## Begin to track the process
        t0 = time.time()
        bun = 0
        for i in xrange(N_t):
            # Check radius
            if type(radius) == np.ndarray:
                r = radius[i]
            ## Obtaining neighs of a given point
            point_i = locs[indices[i], :]
            neighs = kdtree.query_ball_point(point_i, r)
            ## Loop over the possible reindices
            for k in range(n_calc):
                # Retrieve local characterizers
                val_i, neighs_k, vals =\
                    retrieve_local_caracterizers(i, k, neighs, type_arr,
                                                 reindices, agg_desc)
                # Computation of the local measure
                corr_loc_i = self.compute_from_descriptors(vals, val_i, n_vals,
                                                           N_x)
                # Aggregation
                corr_loc[val_i, :, k] += corr_loc_i
            ## Finish to track this process
            if bool_inform and (i % self.lim_rows) == 0 and i != 0:
                t_sp = time.time()-t0
                bun += 1
                self.logfile.write_log(message2a % (bun, self.lim_rows, t_sp))
                t0 = time.time()
        return corr_loc

    def compute_nets_parallel_data():
        pass

    def compute_nets_parallel_neighs():
        pass

    ###########################################################################
    ########################### Matrix computation ############################
    ###########################################################################
    def compute_matrix(self, df, type_var, loc_vars, radius, permuts=None):
        ## 0. Setting needed variables
        self.logfile.write_log(message0 % self.neighs_dir)
        t00 = time.time()
        # Preparing needed vars
        aux = preparing_net_computation(df, type_var, self.lim_rows, permuts)
        type_arr, type_vals, n_vals, N_t, N_x = aux[:5]
        reindices, n_calc, bool_inform = aux[5:]
        # KDTree retrieve object instantiation
        locs = df[loc_vars].as_matrix()
        kdtree = KDTree(locs, leafsize=10000)
        radius = radius/6371.009
        if type(radius) == float:
            r = radius
        elif type(radius) == str:
            radius = np.array(df[radius])

        ## 1. Computation of the local spatial correlation with M-index
        corr_loc = []
        indices = np.array(df.index)
        global_nfo_desc = self.compute_global_info_descriptor(n_vals, N_t, N_x)
        ## Begin to track the process
        t0 = time.time()
        bun = 0
        for i in xrange(N_t):
            # Check radius
            if type(radius) == np.ndarray:
                r = radius[i]
            ## Obtaining neighs of a given point
            point_i = locs[indices[i], :]
            neighs = kdtree.query_ball_point(point_i, r)
            ## Loop over the possible reindices
            for k in range(n_calc):
                # Retrieve local characterizers
                val_i, neighs_k, vals =\
                    retrieve_local_caracterizers(i, k, neighs, type_arr,
                                                 reindices, type_arr)
                # Computation of the descriptors
                descriptors = self.compute_local_descriptors(val_i, vals,
                                                             n_vals, N_x,
                                                             **global_nfo_desc)
                corr_loc.append(descriptors)
            ## Finish to track this process
            if bool_inform and (i % self.lim_rows) == 0 and i != 0:
                t_sp = time.time()-t0
                bun += 1
                self.logfile.write_log(message2a % (bun, self.lim_rows, t_sp))
                t0 = time.time()
        ## Closing process
        t_expended = time.time()-t00
        self.logfile.write_log(message3 % t_expended)
        self.logfile.write_log(message_close)
        self.time_expended = t_expended
        return corr_loc, type_vals, N_x